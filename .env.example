# Cyber-AutoAgent Environment Configuration Template
# Copy this file to .env and customize for your environment

# RECOMMENDATION: keep the .env file lean by focusing on AWS_* access credentials and
# potentially remote stores (if you use them) and otherwise rely on the agent default values
# or modify them via the config.py

# ==============================================================================
# MODEL PROVIDER CONFIGURATION
# ==============================================================================

# Choose your model provider: 'bedrock' (AWS Bedrock), 'ollama' (Local), 'litellm' (Universal), or 'anthropic_oauth' (Claude Max)
# Default: bedrock
# anthropic_oauth uses OAuth to bill against Claude Max unlimited quota instead of API usage
# CYBER_AGENT_PROVIDER=bedrock

# ==============================================================================
# AWS BEDROCK CONFIGURATION (Direct or via LiteLLM)
# ==============================================================================

# AWS Credentials for Bedrock access
# Required for bedrock provider - use either standard credentials OR bearer token
# Option 1: Standard AWS credentials
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key

# Option 2: AWS Bedrock API key (bearer token)
AWS_BEARER_TOKEN_BEDROCK=your_bearer_token

AWS_REGION=us-east-1

# Override default LLM model
# Default: us.anthropic.claude-sonnet-4-5-20250929-v1:0
# CYBER_AGENT_LLM_MODEL=us.anthropic.claude-sonnet-4-20250514-v1:0

# Override default embedding model
# Default: amazon.titan-embed-text-v2:0
# CYBER_AGENT_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0

# Override evaluation model
# Default: us.anthropic.claude-3-5-sonnet-20241022-v2:0
# CYBER_AGENT_EVALUATION_MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0
# RAGAS_EVALUATOR_MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0

# Override swarm model
# Default: us.anthropic.claude-3-5-sonnet-20241022-v2:0
# CYBER_AGENT_SWARM_MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0

# ==============================================================================
# ANTHROPIC OAUTH CONFIGURATION (Claude Max Billing)
# ==============================================================================

# Use OAuth authentication to bill against Claude Max unlimited quota.
# No API key needed - will prompt for authentication on first run.
# Token is stored in ~/.config/cyber-autoagent/.claude_oauth

# Set provider to anthropic_oauth
# CYBER_AGENT_PROVIDER=anthropic_oauth

# Override default LLM model
# Default: claude-sonnet-4-20250514
# CYBER_AGENT_LLM_MODEL=claude-sonnet-4-20250514

# Anthropic doesn't provide embeddings - use local or alternative
# Default: multi-qa-MiniLM-L6-cos-v1 (requires: pip install sentence-transformers)
# CYBER_AGENT_EMBEDDING_MODEL=multi-qa-MiniLM-L6-cos-v1

# ==============================================================================
# LITELLM CONFIGURATION (Universal Provider)
# ==============================================================================

# LiteLLM supports 100+ model providers via unified interface.
# Model format: provider/model-name (e.g., bedrock/..., openai/..., xai/grok-4-latest)

# Example: X.AI Grok 4 for reasoning with Titan embeddings handled by Bedrock
# CYBER_AGENT_PROVIDER=litellm
# CYBER_AGENT_LLM_MODEL=xai/grok-4-latest
# XAI_API_KEY=your_xai_api_key
# CYBER_AGENT_EMBEDDING_MODEL=bedrock/amazon.titan-embed-text-v2:0
# AWS_ACCESS_KEY_ID=...
# AWS_SECRET_ACCESS_KEY=...

# Example: Gemini via LiteLLM (requires google-genai dependency)
# CYBER_AGENT_LLM_MODEL=gemini/gemini-2.5-flash
# GEMINI_API_KEY=your_gemini_api_key

# Example: OpenAI via LiteLLM
# CYBER_AGENT_LLM_MODEL=openai/gpt-4o
# OPENAI_API_KEY=your_openai_api_key

# ==============================================================================
# OLLAMA CONFIGURATION (Local Provider)
# ==============================================================================

# Ollama server host
# Default: auto-detected (localhost or host.docker.internal)
# OLLAMA_HOST=http://localhost:11434

# Override default LLM model for local mode
# Default: llama3.2:3b
# CYBER_AGENT_LLM_MODEL=llama3.2:3b

# Override default embedding model for local mode
# Default: mxbai-embed-large
# CYBER_AGENT_EMBEDDING_MODEL=mxbai-embed-large

# ==============================================================================
# MEMORY SYSTEM CONFIGURATION
# ==============================================================================

# Memory backend provider configuration
# Options: FAISS (default), OpenSearch, Mem0 Platform

# Mem0 Platform (cloud-based memory)
# Get your API key from https://mem0.ai
# MEM0_API_KEY=your_mem0_api_key

# OpenSearch backend
# OPENSEARCH_HOST=https://your-opensearch-domain.region.es.amazonaws.com

# Override memory LLM model
# MEM0_LLM_MODEL=llama3.2:3b

# Override memory embedding model
# MEM0_EMBEDDING_MODEL=mxbai-embed-large

# ==============================================================================
# OUTPUT DIRECTORY CONFIGURATION
# ==============================================================================

# Base directory for all output artifacts
# Default: ./outputs (relative to project root)
# CYBER_AGENT_OUTPUT_DIR=/custom/outputs

# Enable unified output directory structure
# Default: true (recommended)
# CYBER_AGENT_ENABLE_UNIFIED_OUTPUT=true

# ==============================================================================
# OBSERVABILITY & MONITORING
# ==============================================================================

# Enable/disable observability tracing
# Default: true
# ENABLE_OBSERVABILITY=true

# Langfuse configuration for observability
# LANGFUSE_HOST is auto-configured by Docker Compose - only set for custom instances
# LANGFUSE_HOST=http://your-custom-langfuse:3000
LANGFUSE_PUBLIC_KEY=cyber-public
LANGFUSE_SECRET_KEY=cyber-secret

# Langfuse security settings (for production)
# LANGFUSE_ENCRYPTION_KEY=your-32-char-hex-key
# LANGFUSE_SALT=your-16-char-hex-salt
# LANGFUSE_ADMIN_PASSWORD=your-strong-password

# Enable debug trace output to console
# Default: false
# DEBUG_TRACES=true

# ==============================================================================
# PROMPT MANAGEMENT (LANGFUSE)
# ==============================================================================

# Enable Langfuse prompt management for dynamic prompt updates
# Default: true (prompts are auto-created on first run)
ENABLE_LANGFUSE_PROMPTS=true

# Langfuse prompt label to use (production, staging, dev)
# Default: production
LANGFUSE_PROMPT_LABEL=production

# Cache TTL for prompts in seconds
# Default: 300 (5 minutes)
LANGFUSE_PROMPT_CACHE_TTL=300

# ==============================================================================
# EVALUATION SYSTEM
# ==============================================================================

# Enable automatic evaluation after operations
# Default: false
# ENABLE_AUTO_EVALUATION=true

# ==============================================================================
# DEVELOPMENT & DEBUGGING
# ==============================================================================

# Development mode
# Default: false
# DEV=true

# Bypass tool consent prompts
# Default: false (prompts enabled)
# BYPASS_TOOL_CONSENT=true

# ==============================================================================
# EXAMPLE CONFIGURATIONS
# ==============================================================================

# Example 1: Local development with Ollama
# CYBER_AGENT_SERVER=local
# OLLAMA_HOST=http://localhost:11434
# CYBER_AGENT_OUTPUT_DIR=./outputs
# ENABLE_OBSERVABILITY=true

# Example 2: Remote production with AWS Bedrock
# CYBER_AGENT_SERVER=remote
# AWS_ACCESS_KEY_ID=your_key
# AWS_SECRET_ACCESS_KEY=your_secret
# AWS_REGION=us-east-1
# CYBER_AGENT_OUTPUT_DIR=/app/outputs
# ENABLE_AUTO_EVALUATION=true
# MEM0_API_KEY=your_mem0_key

# Example 3: Container deployment with unified output
# CYBER_AGENT_ENABLE_UNIFIED_OUTPUT=true
# CYBER_AGENT_OUTPUT_DIR=/app/outputs
# ENABLE_OBSERVABILITY=true
